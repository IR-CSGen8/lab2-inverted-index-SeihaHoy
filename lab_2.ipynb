{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d8d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv(\"semi_strut.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "# df.to_json(\"semi_strut.json\", orient=\"records\")\n",
    "\n",
    "# js=pd.read_json('semi_strut.json')\n",
    "\n",
    "# print(df.to_string()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03b5ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting\n",
      "Started\n",
      "Python\n",
      "is\n",
      "a\n",
      "versatile\n",
      "programming\n",
      "language...\n",
      "Basic\n",
      "Syntax\n",
      "Python\n",
      "syntax\n",
      "is\n",
      "easy\n",
      "to\n",
      "understand...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_documents = json.loads(df[\"Content\"][0])\n",
    "for section in json_documents['sections']:\n",
    "    # print(section['title'])\n",
    "    for title in section['title'].split():\n",
    "        print(title)\n",
    "    for content in section['content'].split():\n",
    "        print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72eeade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ea1871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'python',\n",
       " 'john',\n",
       " 'doe',\n",
       " 'programming',\n",
       " 'beginner',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'python',\n",
       " 'versatile',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'basic',\n",
       " 'syntax',\n",
       " 'python',\n",
       " 'syntax',\n",
       " 'easy',\n",
       " 'understand']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization function to extract terms from the JSON-like content\n",
    "# Remember to exact both \n",
    "# 1 .Extract terms from various fields (title, keywords, and content)\n",
    "# 2. Extract terms from sections' titles and content\n",
    "# 3. Tokenize the content and create a new column \"Terms\"\n",
    "# 4. Implement a preprocessing function that converts terms to lowercase, removes punctuation, and removes common stop words.\n",
    "    # Create another new column \"Terms_preprocessed\"\n",
    "# 5. you can display the DataFrame\n",
    "# def tokenize_content(content):\n",
    "\n",
    "key_documents = list(df[\"Content\"])\n",
    "# print(len(key_documents))\n",
    "# [key_documents]\n",
    "def tokenize_content(key_documents):\n",
    "    terms = []\n",
    "    # for key_document in key_documents:\n",
    "    json_file = json.loads(key_documents)\n",
    "    # terms.append(json_file['title'])\n",
    "\n",
    "    for unique_title in json_file['title'].split():\n",
    "        if unique_title not in terms:\n",
    "            terms.append(unique_title)\n",
    "    \n",
    "    for unique_author in json_file['author'].split():\n",
    "        if unique_author not in terms:\n",
    "            terms.append(unique_author)\n",
    "\n",
    "\n",
    "    for keywords in json_file['keywords']:\n",
    "        for unique_keywords in keywords.split():\n",
    "            if unique_keywords not in terms:\n",
    "                terms.append(unique_keywords)\n",
    "    for section in json_file['sections']:\n",
    "    # print(section['title'])\n",
    "        for title in section['title'].split():\n",
    "            terms.append(title)\n",
    "        for content in section['content'].split():\n",
    "            terms.append(content)\n",
    "    \n",
    "    for term in terms:\n",
    "        term = term.lower()\n",
    "        term = term.translate(str.maketrans('', '', string.punctuation)) \n",
    "\n",
    "        \n",
    "\n",
    "    return preprocess(terms)\n",
    "    \n",
    "terms = tokenize_content(df['Content'][0])\n",
    "terms\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(terms):\n",
    "    terms_preprocessed = []\n",
    "    for term in terms:\n",
    "        term = term.lower()\n",
    "        term = term.translate(str.maketrans('', '', string.punctuation)) \n",
    "        if term not in stop_words:\n",
    "            terms_preprocessed.append(term)\n",
    "    return terms_preprocessed\n",
    "\n",
    "processed_terms = preprocess(terms)\n",
    "processed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27def6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Python\n",
      "Data Analysis with Pandas\n",
      "Web Development with Flask\n",
      "Machine Learning with Scikit-Learn\n",
      "Data Visualization with Matplotlib\n"
     ]
    }
   ],
   "source": [
    "data = df['Content']\n",
    "for d in data:\n",
    "    data_1 = json.loads(d)\n",
    "    print(data_1['title'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7623ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Terms content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{\\n   \"title\": \"Introduction to Python\",\\n   \"...</td>\n",
       "      <td>[introduction, python, john, doe, programming,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{\\n   \"title\": \"Data Analysis with Pandas\",\\n ...</td>\n",
       "      <td>[data, analysis, pandas, jane, smith, python, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{\\n   \"title\": \"Web Development with Flask\",\\n...</td>\n",
       "      <td>[web, development, flask, mike, johnson, pytho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>{\\n   \"title\": \"Machine Learning with Scikit-L...</td>\n",
       "      <td>[machine, learning, scikitlearn, emily, davis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{\\n   \"title\": \"Data Visualization with Matplo...</td>\n",
       "      <td>[data, visualization, matplotlib, robert, clar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document ID                                            Content  \\\n",
       "0            1  {\\n   \"title\": \"Introduction to Python\",\\n   \"...   \n",
       "1            2  {\\n   \"title\": \"Data Analysis with Pandas\",\\n ...   \n",
       "2            3  {\\n   \"title\": \"Web Development with Flask\",\\n...   \n",
       "3            4  {\\n   \"title\": \"Machine Learning with Scikit-L...   \n",
       "4            5  {\\n   \"title\": \"Data Visualization with Matplo...   \n",
       "\n",
       "                                       Terms content  \n",
       "0  [introduction, python, john, doe, programming,...  \n",
       "1  [data, analysis, pandas, jane, smith, python, ...  \n",
       "2  [web, development, flask, mike, johnson, pytho...  \n",
       "3  [machine, learning, scikitlearn, emily, davis,...  \n",
       "4  [data, visualization, matplotlib, robert, clar...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Terms content\"] = df[\"Content\"].apply(tokenize_content)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131fdcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'introduction': {0, 1, 3, 4},\n",
       " 'python': {0, 1, 2, 3, 4},\n",
       " 'john': {0},\n",
       " 'doe': {0},\n",
       " 'programming': {0},\n",
       " 'beginner': {0},\n",
       " 'getting': {0, 2},\n",
       " 'started': {0, 2},\n",
       " 'versatile': {0, 4},\n",
       " 'language': {0},\n",
       " 'basic': {0},\n",
       " 'syntax': {0},\n",
       " 'easy': {0},\n",
       " 'understand': {0},\n",
       " 'data': {1, 4},\n",
       " 'analysis': {1},\n",
       " 'pandas': {1},\n",
       " 'jane': {1},\n",
       " 'smith': {1},\n",
       " 'popular': {1},\n",
       " 'library': {1, 4},\n",
       " 'dataframes': {1},\n",
       " 'core': {1},\n",
       " 'structure': {1},\n",
       " 'web': {2},\n",
       " 'development': {2},\n",
       " 'flask': {2},\n",
       " 'mike': {2},\n",
       " 'johnson': {2},\n",
       " 'lightweight': {2},\n",
       " 'framework': {2},\n",
       " 'routing': {2},\n",
       " 'defines': {2},\n",
       " 'url': {2},\n",
       " 'patterns': {2},\n",
       " 'views': {2},\n",
       " 'machine': {3},\n",
       " 'learning': {3},\n",
       " 'scikitlearn': {3},\n",
       " 'emily': {3},\n",
       " 'davis': {3},\n",
       " 'subfield': {3},\n",
       " 'artificial': {3},\n",
       " 'intelligence': {3},\n",
       " 'supervised': {3},\n",
       " 'type': {3},\n",
       " 'visualization': {4},\n",
       " 'matplotlib': {4},\n",
       " 'robert': {4},\n",
       " 'clark': {4},\n",
       " 'creating': {4},\n",
       " 'plots': {4},\n",
       " 'created': {4},\n",
       " 'using': {4},\n",
       " 'various': {4},\n",
       " 'functions': {4}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty inverted index dictionary\n",
    "# Build the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "for i, doc in enumerate(df[\"Terms content\"]):\n",
    "    for term in doc:\n",
    "        if term in inverted_index:\n",
    "            inverted_index[term].add(i)\n",
    "        else: inverted_index[term] = {i}\n",
    "\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4824b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty inverted index dictionary the extracted terms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b929397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform boolean operations on postings lists for Boolean search operations\n",
    "# 1. \"Python\" OR \"Pandas\"\n",
    "def or_postings(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            result.append(posting2[p2])\n",
    "            p2 += 1\n",
    "        else:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "    while p1 < len(posting1):\n",
    "        result.append(posting1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(posting2):\n",
    "        result.append(posting2[p2])\n",
    "        p2 += 1\n",
    "    return result\n",
    "\n",
    "pl_1 = list(inverted_index['python'])\n",
    "pl_2 = list(inverted_index['pandas'])\n",
    "or_postings(pl_1, pl_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80680485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. \"Python\" AND \"data\"\n",
    "def and_posting(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            p2 += 1\n",
    "        else:\n",
    "            p1 += 1\n",
    "    return result\n",
    "\n",
    "pl_1 = list(inverted_index['python'])\n",
    "pl_2 = list(inverted_index['pandas'])\n",
    "and_posting(pl_1, pl_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
